---
title: 'RubixKube in Action'
description: 'Real-world scenario - Watch RubixKube detect multiple failures and provide comprehensive analysis'
---

# RubixKube in Action: Multi-Failure Detection

See RubixKube handle **multiple simultaneous failures** - just like real production incidents. This tutorial demonstrates the power of the Agent Mesh and Memory Engine working together.

<Info>
**Real Scenario:** We created 3 different pod failures at once. RubixKube detected all of them, prioritized by severity, and provided specific guidance for each.
</Info>

---

## The Production-Like Scenario

In our demo, we deployed 3 pods that would fail in different ways:

<CardGroup cols={3}>
  <Card title="ImagePullBackOff" icon="image">
    **broken-image-demo**
    
    Invalid container registry - image doesn't exist
  </Card>
  
  <Card title="OOMKilled" icon="memory">
    **memory-hog-demo**
    
    Memory limit 50Mi, but needs 100Mi - gets killed repeatedly
  </Card>
  
  <Card title="CrashLoop" icon="rotate">
    **crash-loop-demo**
    
    Application exits with code 1 - crashes on startup
  </Card>
</CardGroup>

This simulates a real incident where **multiple things go wrong at once** (bad deployment, resource misconfiguration, application bug).

---

## RubixKube's Response (Real Data)

Within **2 minutes**, RubixKube detected ALL THREE failures:

<Frame>
  <img
    style={{ borderRadius: '0.5rem' }}
    src="/images/tutorials/tutorial-02-dashboard-with-failures.png"
    alt="Dashboard showing 3 active insights"
  />
</Frame>

### What Happened

**Dashboard Metrics Changed:**
- **Active Insights:** 0 ‚Üí **3** (three separate issues detected)
- **Notifications:** 0 ‚Üí **20+** (detailed event stream)
- **System Health:** 100% (overall cluster still healthy - isolated failures)
- **Agents:** 3/3 Active (all AI agents working)

**Activity Feed Populated:**
```
üî¥ Container experiencing repeated crashes in crash-loop-demo
   Severity: medium | 2 minutes ago

üî¥ Container experiencing repeated crashes in memory-hog-demo  
   Severity: medium | 2 minutes ago

‚ö†Ô∏è Out of memory (OOMKilled) detected on Pod/memory-hog-demo
   Severity: HIGH | 2 minutes ago
```

<Note>
**Intelligent Prioritization:** RubixKube marked the OOMKilled as **HIGH** severity because it's a resource exhaustion issue. The crash loops are **medium** because they're isolated pod failures.
</Note>

---

## Detailed Incident Analysis

Navigate to **Insights** to see comprehensive analysis:

<Frame>
  <img
    style={{ borderRadius: '0.5rem' }}
    src="/images/tutorials/tutorial-03-insights-with-detections.png"
    alt="Insights showing all 3 incidents with analysis"
  />
</Frame>

### Incident #1: CrashLoop in crash-loop-demo

**Details:**
- **Type:** CrashLoopBackOff
- **Restart Count:** 3 (and increasing)
- **Severity:** Medium
- **RCA Status:** IN_PROGRESS (60% analyzed)

**AI Suggestions:**
- ‚úÖ Check container logs for error messages
- ‚úÖ Verify application configuration
- ‚úÖ Consider increasing resource limits
- ‚úÖ Check for external dependencies

**Source Events:**
```
CrashLoopBackOff: container app in pod crash-loop-demo restarted 3 times
Namespace: rubixkube-tutorials
```

### Incident #2: OOMKilled in memory-hog-demo

**Details:**
- **Type:** Out of Memory
- **Restart Count:** 3
- **Severity:** HIGH üî¥
- **Memory Limit:** 50Mi (too low)

**Root Cause:**
- Container attempted to allocate ~100Mi
- Memory limit set to 50Mi
- Kubernetes killed container to protect node

**Suggested Fix:**
- Increase memory limit to 150Mi (50% buffer over observed usage)
- Monitor for memory leaks
- Consider HPA for auto-scaling

### Incident #3: CrashLoop in memory-hog-demo

**Related Incident:**
- This crash loop is **caused by** the OOMKilled
- RubixKube shows the correlation
- Fixing the memory issue resolves both

<Tip>
**This is intelligent analysis!** RubixKube doesn't just list errors - it understands relationships between incidents.
</Tip>

---

## Agent Mesh Collaboration

Behind the scenes, multiple agents worked together:

<Steps>
  <Step title="Observer Agent">
    **Detected** all 3 pod failures within 30 seconds of each occurring
    
    Sent events to RCA Pipeline Agent for analysis
  </Step>

  <Step title="RCA Pipeline Agent">
    **Analyzed** each incident:
    - Gathered logs, events, pod specs
    - Identified root causes
    - Calculated severity
    - Generated suggestions
  </Step>

  <Step title="Memory Agent">
    **Recalled** similar past incidents (if any existed)
    
    Stored these new incidents for future pattern matching
  </Step>

  <Step title="SRI Agent">
    **Made insights available** via:
    - Dashboard Activity Feed
    - Insights page with details
    - Chat interface for queries
  </Step>
</Steps>

**Total analysis time: 60-90 seconds for all 3 incidents**

---

## Key Observations

### 1. Parallel Detection

**All 3 failures detected simultaneously** - no delay between them:

| Incident | Detection Time | Severity | RCA Progress |
|----------|---------------|----------|--------------|
| crash-loop-demo | 01:58:00 AM | Medium | 60% |
| memory-hog-demo (crash) | 01:58:05 AM | Medium | 60% |
| memory-hog-demo (OOM) | 01:58:10 AM | HIGH | 80% |

**Takeaway:** RubixKube scales - handles multiple incidents without degradation.

### 2. Severity Prioritization

**RubixKube ranked issues appropriately:**
- **HIGH:** OOMKilled (resource exhaustion, node impact risk)
- **Medium:** CrashLoops (isolated pod failures)

**In production:** Focus on HIGH severity first.

### 3. Event Correlation

**RubixKube connected related incidents:**
- Recognized memory-hog-demo's crash loop is **caused by** OOMKilled
- Suggested fixing the root cause (memory) would resolve both

**This prevents wasted effort** fixing symptoms instead of causes.

---

## Real-World Parallels

This scenario mirrors actual production incidents:

<AccordionGroup>
  <Accordion title="Bad Deployment Scenario">
    **Real example:**
    - New version deployed with typo in image tag (ImagePullBackOff)
    - Same version has memory leak (OOMKilled after 10 minutes)
    - Database connection pool exhausted (CrashLoop)
    
    **RubixKube would:**
    - Detect all 3 issues
    - Prioritize by severity
    - Show which are related
    - Suggest rollback to previous version
  </Accordion>

  <Accordion title="Resource Misconfiguration">
    **Real example:**
    - HPA scaled deployment to 10 replicas
    - Namespace quota only allows 8
    - 2 pods stuck pending
    - Remaining 8 pods OOMKilling due to increased load
    
    **RubixKube would:**
    - Detect quota limit
    - Identify OOM root cause
    - Suggest quota increase OR replica reduction
    - Show which pods are affected
  </Accordion>

  <Accordion title="Cascading Failure">
    **Real example:**
    - Database pod OOMKilled
    - API pods start failing (can't connect to DB)
    - Frontend shows 500 errors
    - Load balancer marks all backends unhealthy
    
    **RubixKube would:**
    - Trace the cascade from DB ‚Üí API ‚Üí Frontend
    - Identify DB memory as root cause
    - Show dependency graph
    - Suggest fixing DB first (not the symptoms)
  </Accordion>
</AccordionGroup>

---

## Cleanup

Remove the demo pods:

```bash
kubectl delete namespace rubixkube-tutorials
```

**What happens in RubixKube:**
- Incidents marked as "Resolved"
- Activity Feed shows cleanup
- Active Insights returns to 0
- **Memory retains the learnings** for future incidents

---

## What You Learned

<CardGroup cols={2}>
  <Card title="Multi-Incident Handling" icon="layer-group">
    RubixKube detects and analyzes multiple failures simultaneously
  </Card>
  
  <Card title="Intelligent Prioritization" icon="sort">
    Severity ranking helps you focus on critical issues first
  </Card>
  
  <Card title="Event Correlation" icon="link">
    Related incidents are connected - fix root causes, not symptoms
  </Card>
  
  <Card title="Comprehensive Analysis" icon="magnifying-glass-chart">
    Each incident gets detailed RCA with suggestions
  </Card>
</CardGroup>

---

## Next Steps

<CardGroup cols={2}>
  <Card 
    title="Query Your Infrastructure" 
    icon="comments"
    href="/tutorials/talk-to-infra"
  >
    Learn to use the Chat interface for natural language queries
  </Card>
  
  <Card 
    title="Understand the Memory Engine" 
    icon="database"
    href="/concepts/memory-engine"
  >
    See how RubixKube learns from these incidents
  </Card>
  
  <Card 
    title="Explore Agent Mesh" 
    icon="diagram-project"
    href="/concepts/agent-mesh"
  >
    Learn how agents collaborated to analyze these failures
  </Card>
  
  <Card 
    title="Installation Guide" 
    icon="download"
    href="/getting-started/installation-kind"
  >
    Install RubixKube on your own cluster to try this
  </Card>
</CardGroup>

